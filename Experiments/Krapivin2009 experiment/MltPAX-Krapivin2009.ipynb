{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#please specifiy the path of input dataset: \n",
    "input_dataset= './data/krapivin2009/test/'\n",
    "\n",
    "Path(\"../Output/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../Output/PKE/\").mkdir(parents=True,\n",
    "                             exist_ok=True)  # present keyphrase output\n",
    "Path(\"../Output/AKE/\").mkdir(parents=True,\n",
    "                             exist_ok=True)  # absent keyphrase output\n",
    "Path(\"../Output/entities-URIs/\").mkdir(parents=True,\n",
    "                             exist_ok=True)  # for saving output of MAG (linked entities URIs) \n",
    "\n",
    "Path(\"../Output/AKE-babelnet/\").mkdir(parents=True,\n",
    "                                      exist_ok=True)  # absent keyphrase from BabelNet\n",
    "Path(\"../Output/AKE-DBpedia/\").mkdir(parents=True,\n",
    "                                     exist_ok=True)  # absent keyphrase from BabelNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Present Keyphrase Generation (PKE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keybert import KeyBERT # for present keyphrase extraction\n",
    "import requests\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from py_babelnet.calls import BabelnetAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "\n",
    "#iterate over all files in the dataset ... \n",
    "fNames= glob.glob(input_dataset+'*.txt')\n",
    "\n",
    "for file in fNames:\n",
    "\n",
    "    # read the content of the input document.\n",
    "    file='./Krapivin_Abstracts/'+file.split('/')[-1][:-4]+'.abstr'\n",
    "    \n",
    "    input_doc = open(file, mode='r').read()\n",
    "    input_doc=input_doc.replace('\\t', ' ').replace('\\n', '')\n",
    "    \n",
    "    # extract present keyphrases\n",
    "    keywords = kw_model.extract_keywords(input_doc, keyphrase_ngram_range=(2, 4), \n",
    "                                     stop_words='english')\n",
    "    \n",
    "    # save keywods without relevance score into file\n",
    "    final_keywords=\"\"\n",
    "    for keyword in keywords: \n",
    "        final_keywords+=keyword[0]+\"\\n\"\n",
    "    \n",
    "    with open('./Output/PKE/'+file.split('/')[-1], 'w') as outFile:\n",
    "        outFile.writelines(final_keywords.rstrip())\n",
    "    outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absent Keyphrase Genration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process keyphrases to link with DBpedia based on ngram matching\n",
    "def preprocess_keywords_ngrams(inputFile):\n",
    "    present_keyphrase = pd.read_csv(inputFile, header= None)\n",
    "    \n",
    "    keywordsfull= present_keyphrase[0].tolist()  \n",
    "    keywords=[]\n",
    "    for keyword in keywordsfull:\n",
    "        keyword=keyword.replace(\"'\",\"\")\n",
    "        words=keyword.split(\" \")\n",
    "\n",
    "        keywords.append(keyword)\n",
    "        lastindex=len(words)-1\n",
    "        currentlen=len(words)-1\n",
    "        firstind=0\n",
    "        while currentlen>0:\n",
    "            lastind=firstind+currentlen-1\n",
    "            if lastind <= lastindex:\n",
    "                keywords.append(\" \".join(words[firstind:lastind+1]))\n",
    "                firstind=firstind+1\n",
    "            else:\n",
    "                currentlen=currentlen-1\n",
    "                firstind=0\n",
    "    output= \"\"\n",
    "    for word in keywords: \n",
    "        \n",
    "        output+=\"<entity>\"+word+\"</entity> \"\n",
    "                \n",
    "    return output\n",
    "\n",
    "# save named entities URIs from DBpedia into file\n",
    "def save_dict_to_file(dic, fName):\n",
    "    \n",
    "    f = open('./Output/entities-URIs/'+fName,'w')\n",
    "    f.write(str(dic))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Entities URIs using MAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames= glob.glob(\"./Output/PKE/*.txt\")\n",
    "\n",
    "for file in fNames:\n",
    "    keywords = preprocess_keywords_ngrams(file)\n",
    "    #print(keywords)\n",
    "    mydata= 'text={\"agstring\":\"'+keywords+'\",\"maxkeywords\":10,\"topics\":[]}&type=json'\n",
    "    #print(mydata)\n",
    "    resp=requests.post(\"http://localhost:8080/AGDISTIS\",data=mydata)\n",
    "    \n",
    "    json_data= resp.json()\n",
    "    \n",
    "    linked_entities=\"\"\n",
    "    \n",
    "    for url in json_data['topNodes']: \n",
    "        \n",
    "        linked_entities+=url['entityURL']+\"\\n\"\n",
    "        \n",
    "    save_dict_to_file(linked_entities, file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absent Keyphrase Generation with BabelNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup BabelNet API\n",
    "\n",
    "please change this code to your BabelNet access token. More details can be found here \n",
    "https://babelnet.org/guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = BabelnetAPI('6a01c7b8-50a2-4a18-9385-635ab5e8e489')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def babelNet_linking(word):     \n",
    "\n",
    "    word_lemma= lemmatizer.lemmatize(word)\n",
    "    senses = api.get_senses(lemma = word_lemma, pos=\"NOUN\", searchLang = \"EN\")    \n",
    "    related_terms= set()\n",
    "    \n",
    "    for sens in senses: \n",
    "                   \n",
    "        related_term= sens['properties']['fullLemma'].lower() #align all terms in lowercase         \n",
    "        related_term=lemmatizer.lemmatize(related_term)\n",
    "        related_terms.add(related_term)\n",
    "        \n",
    "    return related_terms\n",
    "\n",
    "def save_to_file(list_related_terms, fName):\n",
    "    f = open('./Output/AKE-babelnet/'+fName,'w')\n",
    "    f.write(str(list_related_terms))\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames = glob.glob(\"../Output/entities-URIs/*.txt\")\n",
    "\n",
    "# for each document which may contain linked_entities: \n",
    "for file in fNames:\n",
    "    if os.stat(file).st_size > 0: # skipp empty documents\n",
    "        linking_df = pd.read_csv(file, header= None, on_bad_lines='skip')    \n",
    "        \n",
    "        linked_entities=[x.split('/')[-1] for x in linking_df[0].tolist()]\n",
    "         \n",
    "        for entity in linked_entities:             \n",
    "            related_terms=babelNet_linking(entity)                    \n",
    "\n",
    "        ## save the output of babelNet linking:         \n",
    "        save_to_file(related_terms, file.split('/')[-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absent Keyphrase Generation with DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "Path(\"../Output/AKE-DBpedia/\").mkdir(parents=True,\n",
    "                                     exist_ok=True)  # absent keyphrase from DBpedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linking with DBpedia to get related terms (dct:subject, gold:hpernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "def dbpedia_linking(uri):\n",
    "\n",
    "    ### Execute SPARQL Query to get dct:subjects for a uri ###\n",
    "    sparql.setQuery(\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        SELECT ?subject\n",
    "        WHERE { \"\"\"+uri+\"\"\" dct:subject ?subject }\n",
    "        \"\"\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    related_terms = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        related_terms.append(\n",
    "            result['subject']['value'].split('/')[-1].split(':')[-1])\n",
    "\n",
    "    ### Execute another SPARQL Query to get hypernyms of a uri ###\n",
    "    sparql.setQuery(\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        SELECT ?subject\n",
    "        WHERE { \"\"\"+uri+\"\"\" gold:hypernym ?subject }\n",
    "        \"\"\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        related_terms.append(\n",
    "            result['subject']['value'].split('/')[-1].split(':')[-1])\n",
    "\n",
    "    return related_terms\n",
    "\n",
    "def save_to_file(list_related_terms, fName):\n",
    "    f = open('./Output/AKE-DBpedia/'+fName,'w')\n",
    "    for word in list_related_terms:  \n",
    "        f.write(word+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames = glob.glob(\"./Output/entities-URIs/*.txt\")\n",
    "\n",
    "# for each document which may contain linked_entities:\n",
    "for file in fNames:\n",
    "    if os.stat(file).st_size > 0:  # skipp empty documents\n",
    "        linking_df = pd.read_csv(file, header=None, on_bad_lines='skip')\n",
    "\n",
    "        linked_entities = [x for x in linking_df[0].tolist()]\n",
    "\n",
    "        for uri in linked_entities:\n",
    "            related_terms = dbpedia_linking(\"<\"+uri+\">\")\n",
    "\n",
    "        # save the output of DBpedia linking:\n",
    "        save_to_file(related_terms, file.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Keyphrase Generation (Semantic Matching):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document (abstract) embedding representation from BERT model.\n",
    "# Get words (present and absent keyphrases) embeddding representation from BERT Model.\n",
    "# Compute the cosine similarity between doc2vec and words2vec, then return a sorted list as an output.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# check if the output folder of absent keyphrases already exisit\n",
    "Path(\"./Output/AKE/\").mkdir(parents=True, exist_ok=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Keyphrases(fileName):\n",
    "\n",
    "    DBpedia_AKE = './Output/AKE-DBpedia/'+fileName\n",
    "\n",
    "    dbpedia_keyphrases = open(DBpedia_AKE, mode='r').readlines()\n",
    "    dbpedia_keyphrases = [keyphrase.replace(\n",
    "        '\\n', '') for keyphrase in dbpedia_keyphrases]\n",
    "\n",
    "    # check if there is absent keyphrases for the input file:\n",
    "    babelNet_keyphrases = []\n",
    "\n",
    "    babelNet_AKE = Path('./Output/AKE-babelnet/'+fileName)\n",
    "\n",
    "    if babelNet_AKE.is_file():\n",
    "        babelNet_keyphrases = open(babelNet_AKE, mode='r').read()\n",
    "\n",
    "        babelNet_keyphrases = [\n",
    "            keyphrase[1:-1].replace('\\'', '') for keyphrase in babelNet_keyphrases[1:-1].split(',')]\n",
    "\n",
    "    # return list\n",
    "    absent_keyphrases = dbpedia_keyphrases+babelNet_keyphrases\n",
    "\n",
    "    return absent_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all files in the dataset ... \n",
    "fNames= glob.glob(input_dataset+'*.abstr')\n",
    "\n",
    "for file in fNames:\n",
    "    \n",
    "    # read the content of the input document.\n",
    "    input_doc = open(file, mode='r').read()\n",
    "    input_doc=input_doc.replace('\\t', ' ').replace('\\n', '')        \n",
    "    doc_embedding = model.encode(input_doc, convert_to_tensor=True)\n",
    "\n",
    "    #---- get the predicted keyphrases ---#\n",
    "    fileName= file.split('/')[-1]\n",
    "    \n",
    "    predicted_keyphrases = get_Keyphrases(fileName)\n",
    "    \n",
    "    keyphrase_embedding = model.encode(predicted_keyphrases, convert_to_tensor=True)\n",
    "\n",
    "    #----- Compute cosine-similarits -----#\n",
    "    cosine_scores = util.pytorch_cos_sim(doc_embedding, keyphrase_embedding)\n",
    "\n",
    "    #--- Output the pairs with their score ----#\n",
    "    similar_keyphrases={}\n",
    "    \n",
    "    for i in range(len(predicted_keyphrases)):\n",
    "        similar_keyphrases[predicted_keyphrases[i]]= cosine_scores[0][i]\n",
    "    \n",
    "    sorted_keyphrase=sorted(similar_keyphrases.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    final_keyphrases= [keyphrase[0] for keyphrase in sorted_keyphrase]\n",
    "    \n",
    "    #--- save ranked keyphrases into file ----#    \n",
    "    with open('./Output/AKE/'+file.split('/')[-1], 'w') as outFile:        \n",
    "        outFile.writelines(\"%s\\n\" % keyphrase for keyphrase in final_keyphrases)\n",
    "    outFile.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
