{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keybert import KeyBERT\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from py_babelnet.calls import BabelnetAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the datasets and create output folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please specifiy the path of input dataset:\n",
    "input_dataset = '../data/Inspec/docsutf8/'\n",
    "\n",
    "Path(\"./Output/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./Output/PKE/\").mkdir(parents=True,\n",
    "                             exist_ok=True)  # present keyphrase output\n",
    "Path(\"./Output/AKE/\").mkdir(parents=True,\n",
    "                             exist_ok=True)  # absent keyphrase output\n",
    "Path(\"./Output/entities-URIs/\").mkdir(parents=True,\n",
    "                             exist_ok=True)  # for saving output of MAG (linked entities URIs) \n",
    "\n",
    "Path(\"./Output/AKE-babelnet/\").mkdir(parents=True,\n",
    "                                      exist_ok=True)  # absent keyphrase from BabelNet\n",
    "Path(\"./Output/AKE-DBpedia/\").mkdir(parents=True,\n",
    "                                     exist_ok=True)  # absent keyphrase from BabelNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Present Keyphrase Generation (PKE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to obtain the test dataset for keyphrase evaluation.\n",
    "# Get id of the test documents from the benchmarking path (../ake-datasets/../test)\n",
    "test_docs = glob.glob('../data/Inspec/test/*.xml')\n",
    "test_docs = ['../Inspec/docsutf8/' +\n",
    "             doc.split('/')[-1][:-3]+'txt' for doc in test_docs]\n",
    "\n",
    "kw_model = KeyBERT()  # create a keyphrase extraction model based on BERT model\n",
    "\n",
    "# iterate over all files in the dataset ...\n",
    "for file in test_docs:\n",
    "    # read the content of the input document.\n",
    "    input_doc = open(file, mode='r').read()\n",
    "    input_doc = input_doc.replace('\\t', ' ').replace('\\n', '')\n",
    "\n",
    "    # extract present keyphrases\n",
    "    keywords = kw_model.extract_keywords(input_doc, keyphrase_ngram_range=(2, 4),\n",
    "                                         stop_words='english', top_n=10)\n",
    "\n",
    "    # save keywods without relevance score into file\n",
    "    final_keywords = \"\"\n",
    "    for keyword in keywords:\n",
    "        final_keywords += keyword[0]+\"\\n\"\n",
    "\n",
    "    with open('./Output/PKE/'+file.split('/')[-1], 'w') as outFile:\n",
    "        outFile.writelines(final_keywords.rstrip())\n",
    "    outFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absent Keyphrase Genration (AKE):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process keyphrases to link with DBpedia based on n-gram matching using the MAG system\n",
    "def preprocess_keywords_ngrams(inputFile):\n",
    "    present_keyphrase = pd.read_csv(inputFile, header=None)\n",
    "\n",
    "    # get the list of present keyphrases\n",
    "    keywordsfull = present_keyphrase[0].tolist()\n",
    "    keywords = []\n",
    "\n",
    "    for keyword in keywordsfull:\n",
    "        keyword = keyword.replace(\"'\", \"\")  # clean quotations\n",
    "        words = keyword.split(\" \")\n",
    "\n",
    "        keywords.append(keyword)\n",
    "        lastindex = len(words)-1\n",
    "        currentlen = len(words)-1\n",
    "\n",
    "        firstind = 0\n",
    "\n",
    "        # detect keywords based on n-grams matching\n",
    "\n",
    "        while currentlen > 0:\n",
    "            lastind = firstind+currentlen-1\n",
    "            if lastind <= lastindex:\n",
    "                keywords.append(\" \".join(words[firstind:lastind+1]))\n",
    "                firstind = firstind+1\n",
    "            else:\n",
    "                currentlen = currentlen-1\n",
    "                firstind = 0\n",
    "\n",
    "    output = \"\"\n",
    "\n",
    "    for word in keywords:\n",
    "        output += \"<entity>\"+word+\"</entity> \"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# save named entities URIs from DBpedia into file\n",
    "def save_dict_to_file(dic, fName):\n",
    "    f = open('./Output/entities-URIs/'+fName, 'w')\n",
    "    f.write(str(dic))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Entities URIs using MAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames = glob.glob(\"./Output/PKE/*.txt\")\n",
    "\n",
    "for file in fNames:\n",
    "    keywords = preprocess_keywords_ngrams(file)\n",
    "    mydata = 'text={\"agstring\":\"'+keywords + \\\n",
    "        '\",\"maxkeywords\":10,\"topics\":[]}&type=json'\n",
    "    resp = requests.post(\"http://localhost:8080/AGDISTIS\", data=mydata)\n",
    "\n",
    "    json_data = resp.json()\n",
    "    linked_entities = \"\"\n",
    "\n",
    "    for url in json_data['topNodes']:\n",
    "\n",
    "        linked_entities += url['entityURL']+\"\\n\"\n",
    "\n",
    "    save_dict_to_file(linked_entities, file.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absent Keyphrase Generation with BabelNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup BabelNet API\n",
    "\n",
    "please change this code to your BabelNet access token. More details can be found here \n",
    "https://babelnet.org/guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api = BabelnetAPI('6a01c7b8-50a2-4a18-9385-635ab5e8e489')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def babelNet_linking(word):\n",
    "\n",
    "    word_lemma = lemmatizer.lemmatize(word)\n",
    "    senses = api.get_senses(lemma=word_lemma, pos=\"NOUN\", searchLang=\"EN\")\n",
    "    related_terms = set()\n",
    "\n",
    "    for sens in senses:\n",
    "\n",
    "        # convertl all terms in lowercase\n",
    "        related_term = sens['properties']['fullLemma'].lower()\n",
    "        related_term = lemmatizer.lemmatize(related_term)\n",
    "        related_terms.add(related_term)\n",
    "\n",
    "    return related_terms\n",
    "\n",
    "# save return list of terms into file.\n",
    "def save_to_file(list_related_terms, fName):\n",
    "    f = open('./Output/AKE-babelnet/'+fName, 'w')\n",
    "    f.write(str(list_related_terms))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames = glob.glob(\"./Output/entities-URIs/*.txt\")\n",
    "\n",
    "# for each document which may contain linked_entities:\n",
    "for file in fNames:\n",
    "    if os.stat(file).st_size > 0:  # skipp empty documents\n",
    "        linking_df = pd.read_csv(file, header=None, on_bad_lines='skip')\n",
    "\n",
    "        linked_entities = [x.split('/')[-1] for x in linking_df[0].tolist()]\n",
    "\n",
    "        for entity in linked_entities:\n",
    "            related_terms = babelNet_linking(entity)\n",
    "\n",
    "        # save the output of babelNet linking:\n",
    "        save_to_file(related_terms, file.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absent Keyphrase Generation with DBpedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "Path(\"../Output/AKE-DBpedia/\").mkdir(parents=True,\n",
    "                                     exist_ok=True)  # absent keyphrase from DBpedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linking with DBpedia to get related terms (dct:subject, gold:hpernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "def dbpedia_linking(uri):\n",
    "\n",
    "    ### Execute SPARQL Query to get dct:subjects for a uri ###\n",
    "    sparql.setQuery(\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        SELECT ?subject\n",
    "        WHERE { \"\"\"+uri+\"\"\" dct:subject ?subject }\n",
    "        \"\"\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    related_terms = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        related_terms.append(\n",
    "            result['subject']['value'].split('/')[-1].split(':')[-1])\n",
    "\n",
    "    ### Execute another SPARQL Query to get hypernyms of a uri ###\n",
    "    sparql.setQuery(\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        SELECT ?subject\n",
    "        WHERE { \"\"\"+uri+\"\"\" gold:hypernym ?subject }\n",
    "        \"\"\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        related_terms.append(\n",
    "            result['subject']['value'].split('/')[-1].split(':')[-1])\n",
    "\n",
    "    return related_terms\n",
    "\n",
    "def save_to_file(list_related_terms, fName):\n",
    "    f = open('./Output/AKE-DBpedia/'+fName,'w')\n",
    "    for word in list_related_terms:  \n",
    "        f.write(word+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames = glob.glob(\"./Output/entities-URIs/*.txt\")\n",
    "\n",
    "# for each document which may contain linked_entities:\n",
    "for file in fNames:\n",
    "    if os.stat(file).st_size > 0:  # skipp empty documents\n",
    "        linking_df = pd.read_csv(file, header=None, on_bad_lines='skip')\n",
    "\n",
    "        linked_entities = [x for x in linking_df[0].tolist()]\n",
    "\n",
    "        for uri in linked_entities:\n",
    "            related_terms = dbpedia_linking(\"<\"+uri+\">\")\n",
    "\n",
    "        # save the output of DBpedia linking:\n",
    "        save_to_file(related_terms, file.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Keyphrase Generation (Semantic Matching):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document (abstract) embedding representation from BERT model.\n",
    "# Get words (present and absent keyphrases) embeddding representation from BERT Model.\n",
    "# Compute the cosine similarity between doc2vec and words2vec, then return a sorted list as an output.\n",
    "\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# check if the output folder of absent keyphrases already exisit\n",
    "Path(\"./Output/AKE/\").mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Keyphrases(fileName):\n",
    "\n",
    "    DBpedia_AKE = './Output/AKE-DBpedia/'+fileName\n",
    "\n",
    "    dbpedia_keyphrases = open(DBpedia_AKE, mode='r').readlines()\n",
    "    dbpedia_keyphrases = [keyphrase.replace(\n",
    "        '\\n', '') for keyphrase in dbpedia_keyphrases]\n",
    "\n",
    "    # check if there is absent keyphrases for the input file:\n",
    "    babelNet_keyphrases = []\n",
    "\n",
    "    babelNet_AKE = Path('./Output/AKE-babelnet/'+fileName)\n",
    "\n",
    "    if babelNet_AKE.is_file():\n",
    "        babelNet_keyphrases = open(babelNet_AKE, mode='r').read()\n",
    "\n",
    "        babelNet_keyphrases = [\n",
    "            keyphrase[1:-1].replace('\\'', '') for keyphrase in babelNet_keyphrases[1:-1].split(',')]\n",
    "\n",
    "    # return list\n",
    "    absent_keyphrases = dbpedia_keyphrases+babelNet_keyphrases\n",
    "\n",
    "    return absent_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all files in the dataset ...\n",
    "fNames = glob.glob(input_dataset+'*.txt')\n",
    "\n",
    "for file in fNames:\n",
    "\n",
    "    # read the content of the input document.\n",
    "    input_doc = open(file, mode='r').read()\n",
    "    input_doc = input_doc.replace('\\t', ' ').replace('\\n', '')\n",
    "\n",
    "    #get document embedding vector from the BERT model\n",
    "    doc_embedding = model.encode(input_doc, convert_to_tensor=True)\n",
    "\n",
    "    #---- get the predicted keyphrases ---#\n",
    "    fileName = file.split('/')[-1]\n",
    "\n",
    "    predicted_keyphrases = get_Keyphrases(fileName)\n",
    "\n",
    "    keyphrase_embedding = model.encode(\n",
    "        predicted_keyphrases, convert_to_tensor=True)\n",
    "\n",
    "    #----- Compute cosine-similarits -----#\n",
    "    cosine_scores = util.pytorch_cos_sim(doc_embedding, keyphrase_embedding)\n",
    "\n",
    "    #--- Output the pairs with their score ----#\n",
    "    similar_keyphrases = {}\n",
    "\n",
    "    for i in range(len(predicted_keyphrases)):\n",
    "        similar_keyphrases[predicted_keyphrases[i]] = cosine_scores[0][i]\n",
    "\n",
    "    sorted_keyphrase = sorted(\n",
    "        similar_keyphrases.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    final_keyphrases = [keyphrase[0] for keyphrase in sorted_keyphrase]\n",
    "\n",
    "    #--- save ranked keyphrases into file ----#\n",
    "    with open('./Output/AKE/'+file.split('/')[-1], 'w') as outFile:\n",
    "        outFile.writelines(\"%s\\n\" %\n",
    "                           keyphrase for keyphrase in final_keyphrases)\n",
    "    outFile.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
