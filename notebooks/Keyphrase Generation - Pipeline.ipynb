{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "    \n",
    "from keybert import KeyBERT # for present keyphrase extraction\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from py_babelnet.calls import BabelnetAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#please specifiy the path of input dataset: \n",
    "input_dataset= '../Inspec/docsutf8/'\n",
    "\n",
    "Path(\"./Output/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./Output/PKE/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./Output/AKE/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(\"./Output/AKE-babelnet/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./Output/Ranking/\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Present Keyphrase Generation (PKE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "\n",
    "#iterate over all files in the dataset ... \n",
    "fNames= glob.glob(input_dataset+'*.txt')\n",
    "\n",
    "for file in fNames:\n",
    "    # read the content of the input document.\n",
    "    input_doc = open(file, mode='r').read()\n",
    "    input_doc=input_doc.replace('\\t', ' ').replace('\\n', '')\n",
    "\n",
    "    # extract present keyphrases\n",
    "    keywords = kw_model.extract_keywords(input_doc, keyphrase_ngram_range=(1, 3), \n",
    "                                     stop_words='english')\n",
    "    \n",
    "    # save keywods without relevance score into file\n",
    "    final_keywords=\"\"\n",
    "    for keyword in keywords: \n",
    "        final_keywords+=keyword[0]+\"\\n\"\n",
    "    \n",
    "    with open('./Output/PKE/'+file.split('/')[-1], 'w') as outFile:\n",
    "        outFile.writelines(final_keywords.rstrip())\n",
    "    outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absent Keyphrase Genration (AKE) via Entity Linking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process keyphrases to link with DBpedia based on ngram matching\n",
    "def preprocess_keywords_ngrams(inputFile):\n",
    "    present_keyphrase = pd.read_csv(inputFile, header= None)\n",
    "    \n",
    "    keywordsfull= present_keyphrase[0].tolist()  \n",
    "    keywords=[]\n",
    "    for keyword in keywordsfull:\n",
    "        keyword=keyword.replace(\"'\",\"\")\n",
    "        words=keyword.split(\" \")\n",
    "\n",
    "        keywords.append(keyword)\n",
    "        lastindex=len(words)-1\n",
    "        currentlen=len(words)-1\n",
    "        firstind=0\n",
    "        while currentlen>0:\n",
    "            lastind=firstind+currentlen-1\n",
    "            if lastind <= lastindex:\n",
    "                keywords.append(\" \".join(words[firstind:lastind+1]))\n",
    "                firstind=firstind+1\n",
    "            else:\n",
    "                currentlen=currentlen-1\n",
    "                firstind=0\n",
    "    output= \"\"\n",
    "    for word in keywords: \n",
    "        \n",
    "        output+=\"<entity>\"+word+\"</entity> \"\n",
    "                \n",
    "    return output\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save named entities URIs from DBpedia into file\n",
    "def save_dict_to_file(dic, fName):\n",
    "    \n",
    "    f = open('./Output/AKE/'+fName,'w')\n",
    "    f.write(str(dic))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames= glob.glob(\"./Output/PKE/*.txt\")\n",
    "\n",
    "for file in fNames:\n",
    "    keywords = preprocess_keywords_ngrams(file)\n",
    "    #print(keywords)\n",
    "    mydata= 'text={\"agstring\":\"'+keywords+'\",\"maxkeywords\":10,\"topics\":[]}&type=json'\n",
    "    #print(mydata)\n",
    "    resp=requests.post(\"http://localhost:8080/AGDISTIS\",data=mydata)\n",
    "    \n",
    "    json_data= resp.json()\n",
    "    \n",
    "    linked_entities=\"\"\n",
    "    \n",
    "    for url in json_data['topNodes']: \n",
    "        \n",
    "        linked_entities+=url['entityURL']+\"\\n\"\n",
    "        \n",
    "    save_dict_to_file(linked_entities, file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Keyphrases from BabelNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup BabelNet API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = BabelnetAPI('6a01c7b8-50a2-4a18-9385-635ab5e8e489')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def babelNet_linking(word):     \n",
    "\n",
    "    word_lemma= lemmatizer.lemmatize(word)\n",
    "    senses = api.get_senses(lemma = word_lemma, pos=\"NOUN\", searchLang = \"EN\")    \n",
    "    related_terms= set()\n",
    "    \n",
    "    for sens in senses: \n",
    "                   \n",
    "        related_term= sens['properties']['fullLemma'].lower() #align all terms in lowercase         \n",
    "        related_term=lemmatizer.lemmatize(related_term)\n",
    "        related_terms.add(related_term)\n",
    "        \n",
    "    return related_terms    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(list_related_terms, fName):\n",
    "    \n",
    "    f = open('./Output/AKE-babelnet/'+fName,'w')\n",
    "    f.write(str(list_related_terms))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames= glob.glob(\"./Output/AKE/*.txt\")\n",
    "\n",
    "# for each document which may contain linked_entities: \n",
    "for file in fNames:\n",
    "    if os.stat(file).st_size > 0: # skipp empty documents\n",
    "        linking_df = pd.read_csv(file, header= None, on_bad_lines='skip')    \n",
    "        \n",
    "        linked_entities=[x.split('/')[-1] for x in linking_df[0].tolist()]\n",
    "         \n",
    "        for entity in linked_entities:             \n",
    "            related_terms=babelNet_linking(entity)                    \n",
    "\n",
    "        ## save the output of babelNet linking:         \n",
    "        save_to_file(related_terms, file.split('/')[-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrase Ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document (abstract) embedding representation from BERT model.\n",
    "# Get words (present and absent keyphrases) embeddding representation from BERT Model.\n",
    "# Compute the cosine similarity between doc2vec and words2vec, then return a sorted list as an output.\n",
    "\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ecological_niche',\n",
       " 'environment_(biology)',\n",
       " 'realized_niche',\n",
       " 'niche_(ecology)',\n",
       " 'biological_niche',\n",
       " 'environmental_niche',\n",
       " 'hutchinsonian_niche',\n",
       " 'potential_niche',\n",
       " 'niche',\n",
       " 'adaptive_zone',\n",
       " 'fundamental_niche',\n",
       " 'ecological_niché',\n",
       " 'econiche']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"{'ecological_niche', 'environment_(biology)', 'realized_niche', 'niche_(ecology)', 'biological_niche', 'environmental_niche', 'hutchinsonian_niche', 'potential_niche', 'niche', 'adaptive_zone', 'fundamental_niche', 'ecological_niché', 'econiche'}\"\n",
    "\n",
    "[keyphrase[1:-1].replace('\\'', '') for keyphrase in test[1:-1].split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Keyphrases(fileName): \n",
    "    \n",
    "    PKE_file= './Output/PKE/'+fileName\n",
    "    \n",
    "    presentKeyphrases= open(PKE_file, mode='r').readlines()\n",
    "    \n",
    "    presentKeyphrases= [keyphrase.replace('\\n', '') for keyphrase in presentKeyphrases]\n",
    "        \n",
    "    #check if there is absent keyphrases for the input file: \n",
    "    absentKeyphrases=[]\n",
    "    \n",
    "    AKE_file = Path('./Output/AKE-babelnet/'+fileName)    \n",
    "    \n",
    "    if AKE_file.is_file():\n",
    "        absentKeyphrases= open(AKE_file, mode='r').read()\n",
    "        \n",
    "        absentKeyphrases=[ keyphrase[1:-1].replace('\\'','') for keyphrase in absentKeyphrases[1:-1].split(',') ]                \n",
    "        \n",
    "    final_keyphrases= presentKeyphrases+absentKeyphrases\n",
    "        \n",
    "    \n",
    "    return final_keyphrases    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all files in the dataset ... \n",
    "fNames= glob.glob(input_dataset+'*.txt')\n",
    "\n",
    "for file in fNames:\n",
    "    \n",
    "    # read the content of the input document.\n",
    "    input_doc = open(file, mode='r').read()\n",
    "    input_doc=input_doc.replace('\\t', ' ').replace('\\n', '')        \n",
    "    doc_embedding = model.encode(input_doc, convert_to_tensor=True)\n",
    "\n",
    "    #---- get the predicted keyphrases ---#\n",
    "    fileName= file.split('/')[-1]\n",
    "    \n",
    "    predicted_keyphrases = get_Keyphrases(fileName)\n",
    "    \n",
    "    keyphrase_embedding = model.encode(predicted_keyphrases, convert_to_tensor=True)\n",
    "\n",
    "    #----- Compute cosine-similarits -----#\n",
    "    cosine_scores = util.pytorch_cos_sim(doc_embedding, keyphrase_embedding)\n",
    "\n",
    "    #--- Output the pairs with their score ----#\n",
    "    similar_keyphrases={}\n",
    "    \n",
    "    for i in range(len(predicted_keyphrases)):\n",
    "        similar_keyphrases[predicted_keyphrases[i]]= cosine_scores[0][i]\n",
    "    \n",
    "    sorted_keyphrase=sorted(similar_keyphrases.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    final_keyphrases= [keyphrase[0] for keyphrase in sorted_keyphrase]\n",
    "    \n",
    "    #--- save ranked keyphrases into file ----#    \n",
    "    with open('./Output/Ranking/'+file.split('/')[-1], 'w') as outFile:        \n",
    "        outFile.writelines(\"%s\\n\" % keyphrase for keyphrase in final_keyphrases)\n",
    "    outFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
