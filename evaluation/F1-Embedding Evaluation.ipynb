{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-Embedding Evaluation for Semantic-Matching:\n",
    "\n",
    "- precision: how many matched keyphrases with the ground-truth in the output \n",
    "- recall: how many matched keyphrases in the output w.r.t all ground-truth keyphrases\n",
    "- compute semantic similarity match between two words if cosine_sim > 5.0 ==> consider as a matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"./Output/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./Output/Evaluation/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_writer = csv.writer(open('./Output/Inspec-evaluation.csv',\n",
    "                         'w', encoding='UTF8', newline=''), delimiter='\\t')\n",
    "file_writer.writerow(['Top@', 'precision', 'recall', 'F1'])\n",
    "\n",
    "\n",
    "# Get id of the test documents from the benchmarking path (../ake-datasets/../test)\n",
    "# Modify the path (i.e., redirect to the ground-truth folder) to geth the gold standard keywords\n",
    "test_docs = glob.glob('../ake-datasets/datasets/Inspec/test/*.xml')\n",
    "test_docs = ['../Inspec/keys/' +\n",
    "             doc.split('/')[-1][:-3]+'key' for doc in test_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evaluation(cosine_scores):\n",
    "    \n",
    "    num_of_Similar=0 #number of similar keyphrases, we add 1 to avoid division by zero (i.e., smoothing)\n",
    "\n",
    "    for similarty_score in cosine_scores:\n",
    "        if any(similarity_threshould > 0.5 for similarity_threshould in similarty_score):                \n",
    "            num_of_Similar+=1\n",
    "        \n",
    "    recall= num_of_Similar/len(cosine_scores[0]) # cosine_scores[0] size of ground-truth (see cosine_scores.shape)\n",
    "    precision= num_of_Similar/len(cosine_scores)\n",
    "\n",
    "        \n",
    "    return round(precision, 3), round(recall, 3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "babelNet_path='./Output/AKE-babelnet/' # change this path for the predicted keyphrases\n",
    "present_path='./Output/PKE/'\n",
    "dbpedia_path= './Output/AKE-DBpedia/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Top@20', 0.883, 0.584, 0.703]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_of_recall = 0\n",
    "sum_of_precision = 0\n",
    "\n",
    "#--- K= {3, 5, 10} ---#\n",
    "k = 10\n",
    "\n",
    "file_counts = 0\n",
    "\n",
    "for file in test_docs:\n",
    "\n",
    "    # reading the ground-truth keyphrases as a list\n",
    "    with open(file) as fileIn:\n",
    "        groundtruth_keyphrases = fileIn.readlines()[:k]\n",
    "        groundtruth_keyphrases = [keyphrase.replace(\n",
    "            '\\n', '') for keyphrase in groundtruth_keyphrases]\n",
    "\n",
    "        groundtruth_embedding = model.encode(\n",
    "            groundtruth_keyphrases, convert_to_tensor=True)\n",
    "    fileIn.close()\n",
    "\n",
    "    # reading the generated keyphrases from babelNet as a list\n",
    "    fileName = file.split('/')[-1][:-3]+'txt'\n",
    "    babelNet_file = Path(babelNet_path+fileName)\n",
    "    if not babelNet_file.is_file():\n",
    "        continue\n",
    "\n",
    "    with open(babelNet_path+fileName) as fileIn:\n",
    "        babelNet_keyphrases = fileIn.readlines()[:k]\n",
    "        babelNet_keyphrases = [keyphrase.replace(\n",
    "            '\\n', '') for keyphrase in babelNet_keyphrases]\n",
    "\n",
    "    # reading the generated keyphrases from DBpedia as a list\n",
    "    dbpedia_file = Path(dbpedia_path+fileName)\n",
    "    if not dbpedia_file.is_file():\n",
    "        continue\n",
    "\n",
    "    file_counts += 1\n",
    "\n",
    "    with open(dbpedia_path+fileName) as fileIn:\n",
    "        dbpedia_keyphrases = fileIn.readlines()[:k]\n",
    "        dbpedia_keyphrases = [keyphrase.replace(\n",
    "            '\\n', '') for keyphrase in dbpedia_keyphrases]\n",
    "\n",
    "    # reading the presentk keyphrases as a list\n",
    "    with open(present_path+fileName) as fileIn:\n",
    "        present_keyphrases = fileIn.readlines()[:k]\n",
    "        present_keyphrases = [keyphrase.replace(\n",
    "            '\\n', '') for keyphrase in present_keyphrases]\n",
    "\n",
    "    # combine all keyphrases into a final list\n",
    "    final_keyphrases = present_keyphrases+babelNet_keyphrases+dbpedia_keyphrases\n",
    "\n",
    "    keyphrase_embedding = model.encode(\n",
    "        final_keyphrases[:k], convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.pytorch_cos_sim(\n",
    "        keyphrase_embedding, groundtruth_embedding)\n",
    "\n",
    "    # compute evaluation and save into file\n",
    "    precision, recall = compute_evaluation(cosine_scores)\n",
    "\n",
    "    sum_of_recall += recall\n",
    "    sum_of_precision += precision\n",
    "\n",
    "avg_recall = sum_of_recall/file_counts \n",
    "avg_precision = sum_of_precision/file_counts \n",
    "\n",
    "F1 = 2*(avg_recall*avg_precision)/(avg_precision+avg_recall)\n",
    "\n",
    "print(['Top@'+str(k), round(avg_precision, 3), round(avg_recall, 3), round(F1, 3)])\n",
    "\n",
    "file_writer.writerow(\n",
    "    ['Top@'+str(k), round(avg_precision, 3), round(avg_recall, 3), round(F1, 3)])\n",
    "\n",
    "file_writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
